SGE_HGR_gpu=5
CUDA_VISIBLE_DEVICES=5
Wed May 19 12:49:36 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100S-PCI...  On   | 00000000:1A:00.0 Off |                    0 |
| N/A   33C    P0    58W / 200W |   1401MiB / 32510MiB |     32%   E. Process |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100S-PCI...  On   | 00000000:1B:00.0 Off |                    0 |
| N/A   44C    P0   216W / 200W |  25323MiB / 32510MiB |     70%   E. Process |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100S-PCI...  On   | 00000000:1C:00.0 Off |                    0 |
| N/A   45C    P0   184W / 200W |  29091MiB / 32510MiB |     78%   E. Process |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100S-PCI...  On   | 00000000:3D:00.0 Off |                    0 |
| N/A   46C    P0   199W / 200W |  30627MiB / 32510MiB |     94%   E. Process |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   4  Tesla V100S-PCI...  On   | 00000000:3E:00.0 Off |                    0 |
| N/A   46C    P0   192W / 200W |  30591MiB / 32510MiB |     95%   E. Process |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   5  Tesla V100S-PCI...  On   | 00000000:8B:00.0 Off |                    0 |
| N/A   27C    P0    24W / 200W |      0MiB / 32510MiB |      0%   E. Process |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   6  Tesla V100S-PCI...  On   | 00000000:8C:00.0 Off |                    0 |
| N/A   25C    P0    24W / 200W |      0MiB / 32510MiB |      0%   E. Process |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   7  Tesla V100S-PCI...  On   | 00000000:B4:00.0 Off |                    0 |
| N/A   25C    P0    25W / 200W |      0MiB / 32510MiB |      0%   E. Process |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   8  Tesla V100S-PCI...  On   | 00000000:B5:00.0 Off |                    0 |
| N/A   26C    P0    25W / 200W |      0MiB / 32510MiB |      0%   E. Process |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   9  Tesla V100S-PCI...  On   | 00000000:B6:00.0 Off |                    0 |
| N/A   26C    P0    25W / 200W |      0MiB / 32510MiB |      0%   E. Process |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A    179335      C   python                           1397MiB |
|    1   N/A  N/A    319050      C   ...envs/snowfall2/bin/python    25319MiB |
|    2   N/A  N/A    319051      C   ...envs/snowfall2/bin/python    29087MiB |
|    3   N/A  N/A    319052      C   ...envs/snowfall2/bin/python    30623MiB |
|    4   N/A  N/A    319053      C   ...envs/snowfall2/bin/python    30587MiB |
+-----------------------------------------------------------------------------+
/cm/local/apps/uge/var/spool/r7n04/job_scripts/9177364: Running on r7n04
/cm/local/apps/uge/var/spool/r7n04/job_scripts/9177364: Started at Wed May 19 12:49:37 EDT 2021
/cm/local/apps/uge/var/spool/r7n04/job_scripts/9177364: Running the job on GPU(s) 5
Added key: store_based_barrier_key:1 to store for rank: 0
Loading lexicon and symbol tables
Loading L.fst
About to get train cuts
About to get train cuts
About to get Musan cuts
About to create train dataset
Using cut concatenation with duration factor 1.0 and gap 1.0.
Using SingleCutSampler.
About to create train dataloader
About to get dev cuts
About to get valid cuts
About to create dev dataset
About to create dev dataloader
About to create model
================================================================================
Model parameters summary:
================================================================================
* P_scores:                                                                 7568
* encoder_embed.layers.0.weight:                                             288
* encoder_embed.layers.0.bias:                                                32
* encoder_embed.layers.2.weight:                                            9216
* encoder_embed.layers.2.bias:                                                32
* encoder_embed.layers.4.weight:                                           18432
* encoder_embed.layers.4.bias:                                                64
* encoder_embed.layers.6.weight:                                           36864
* encoder_embed.layers.6.bias:                                                64
* encoder_embed.out.weight:                                               311296
* encoder_embed.out.bias:                                                    256
* encoder.layers.0.self_attn.pos_bias_u:                                     256
* encoder.layers.0.self_attn.pos_bias_v:                                     256
* encoder.layers.0.self_attn.in_proj.weight:                              196608
* encoder.layers.0.self_attn.in_proj.bias:                                   768
* encoder.layers.0.self_attn.out_proj.weight:                              65536
* encoder.layers.0.self_attn.out_proj.bias:                                  256
* encoder.layers.0.self_attn.linear_pos.weight:                            65536
* encoder.layers.0.feed_forward.0.weight:                                 524288
* encoder.layers.0.feed_forward.0.bias:                                     2048
* encoder.layers.0.feed_forward.3.weight:                                 524288
* encoder.layers.0.feed_forward.3.bias:                                      256
* encoder.layers.0.feed_forward_macaron.0.weight:                         524288
* encoder.layers.0.feed_forward_macaron.0.bias:                             2048
* encoder.layers.0.feed_forward_macaron.3.weight:                         524288
* encoder.layers.0.feed_forward_macaron.3.bias:                              256
* encoder.layers.0.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.0.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.0.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.0.conv_module.depthwise_conv.bias:                          256
* encoder.layers.0.conv_module.norm.weight:                                  256
* encoder.layers.0.conv_module.norm.bias:                                    256
* encoder.layers.0.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.0.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.0.norm_ff_macaron.weight:                                   256
* encoder.layers.0.norm_ff_macaron.bias:                                     256
* encoder.layers.0.norm_ff.weight:                                           256
* encoder.layers.0.norm_ff.bias:                                             256
* encoder.layers.0.norm_mha.weight:                                          256
* encoder.layers.0.norm_mha.bias:                                            256
* encoder.layers.0.norm_conv.weight:                                         256
* encoder.layers.0.norm_conv.bias:                                           256
* encoder.layers.0.norm_final.weight:                                        256
* encoder.layers.0.norm_final.bias:                                          256
* encoder.layers.1.self_attn.pos_bias_u:                                     256
* encoder.layers.1.self_attn.pos_bias_v:                                     256
* encoder.layers.1.self_attn.in_proj.weight:                              196608
* encoder.layers.1.self_attn.in_proj.bias:                                   768
* encoder.layers.1.self_attn.out_proj.weight:                              65536
* encoder.layers.1.self_attn.out_proj.bias:                                  256
* encoder.layers.1.self_attn.linear_pos.weight:                            65536
* encoder.layers.1.feed_forward.0.weight:                                 524288
* encoder.layers.1.feed_forward.0.bias:                                     2048
* encoder.layers.1.feed_forward.3.weight:                                 524288
* encoder.layers.1.feed_forward.3.bias:                                      256
* encoder.layers.1.feed_forward_macaron.0.weight:                         524288
* encoder.layers.1.feed_forward_macaron.0.bias:                             2048
* encoder.layers.1.feed_forward_macaron.3.weight:                         524288
* encoder.layers.1.feed_forward_macaron.3.bias:                              256
* encoder.layers.1.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.1.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.1.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.1.conv_module.depthwise_conv.bias:                          256
* encoder.layers.1.conv_module.norm.weight:                                  256
* encoder.layers.1.conv_module.norm.bias:                                    256
* encoder.layers.1.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.1.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.1.norm_ff_macaron.weight:                                   256
* encoder.layers.1.norm_ff_macaron.bias:                                     256
* encoder.layers.1.norm_ff.weight:                                           256
* encoder.layers.1.norm_ff.bias:                                             256
* encoder.layers.1.norm_mha.weight:                                          256
* encoder.layers.1.norm_mha.bias:                                            256
* encoder.layers.1.norm_conv.weight:                                         256
* encoder.layers.1.norm_conv.bias:                                           256
* encoder.layers.1.norm_final.weight:                                        256
* encoder.layers.1.norm_final.bias:                                          256
* encoder.layers.2.self_attn.pos_bias_u:                                     256
* encoder.layers.2.self_attn.pos_bias_v:                                     256
* encoder.layers.2.self_attn.in_proj.weight:                              196608
* encoder.layers.2.self_attn.in_proj.bias:                                   768
* encoder.layers.2.self_attn.out_proj.weight:                              65536
* encoder.layers.2.self_attn.out_proj.bias:                                  256
* encoder.layers.2.self_attn.linear_pos.weight:                            65536
* encoder.layers.2.feed_forward.0.weight:                                 524288
* encoder.layers.2.feed_forward.0.bias:                                     2048
* encoder.layers.2.feed_forward.3.weight:                                 524288
* encoder.layers.2.feed_forward.3.bias:                                      256
* encoder.layers.2.feed_forward_macaron.0.weight:                         524288
* encoder.layers.2.feed_forward_macaron.0.bias:                             2048
* encoder.layers.2.feed_forward_macaron.3.weight:                         524288
* encoder.layers.2.feed_forward_macaron.3.bias:                              256
* encoder.layers.2.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.2.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.2.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.2.conv_module.depthwise_conv.bias:                          256
* encoder.layers.2.conv_module.norm.weight:                                  256
* encoder.layers.2.conv_module.norm.bias:                                    256
* encoder.layers.2.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.2.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.2.norm_ff_macaron.weight:                                   256
* encoder.layers.2.norm_ff_macaron.bias:                                     256
* encoder.layers.2.norm_ff.weight:                                           256
* encoder.layers.2.norm_ff.bias:                                             256
* encoder.layers.2.norm_mha.weight:                                          256
* encoder.layers.2.norm_mha.bias:                                            256
* encoder.layers.2.norm_conv.weight:                                         256
* encoder.layers.2.norm_conv.bias:                                           256
* encoder.layers.2.norm_final.weight:                                        256
* encoder.layers.2.norm_final.bias:                                          256
* encoder.layers.3.self_attn.pos_bias_u:                                     256
* encoder.layers.3.self_attn.pos_bias_v:                                     256
* encoder.layers.3.self_attn.in_proj.weight:                              196608
* encoder.layers.3.self_attn.in_proj.bias:                                   768
* encoder.layers.3.self_attn.out_proj.weight:                              65536
* encoder.layers.3.self_attn.out_proj.bias:                                  256
* encoder.layers.3.self_attn.linear_pos.weight:                            65536
* encoder.layers.3.feed_forward.0.weight:                                 524288
* encoder.layers.3.feed_forward.0.bias:                                     2048
* encoder.layers.3.feed_forward.3.weight:                                 524288
* encoder.layers.3.feed_forward.3.bias:                                      256
* encoder.layers.3.feed_forward_macaron.0.weight:                         524288
* encoder.layers.3.feed_forward_macaron.0.bias:                             2048
* encoder.layers.3.feed_forward_macaron.3.weight:                         524288
* encoder.layers.3.feed_forward_macaron.3.bias:                              256
* encoder.layers.3.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.3.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.3.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.3.conv_module.depthwise_conv.bias:                          256
* encoder.layers.3.conv_module.norm.weight:                                  256
* encoder.layers.3.conv_module.norm.bias:                                    256
* encoder.layers.3.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.3.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.3.norm_ff_macaron.weight:                                   256
* encoder.layers.3.norm_ff_macaron.bias:                                     256
* encoder.layers.3.norm_ff.weight:                                           256
* encoder.layers.3.norm_ff.bias:                                             256
* encoder.layers.3.norm_mha.weight:                                          256
* encoder.layers.3.norm_mha.bias:                                            256
* encoder.layers.3.norm_conv.weight:                                         256
* encoder.layers.3.norm_conv.bias:                                           256
* encoder.layers.3.norm_final.weight:                                        256
* encoder.layers.3.norm_final.bias:                                          256
* encoder.layers.4.self_attn.pos_bias_u:                                     256
* encoder.layers.4.self_attn.pos_bias_v:                                     256
* encoder.layers.4.self_attn.in_proj.weight:                              196608
* encoder.layers.4.self_attn.in_proj.bias:                                   768
* encoder.layers.4.self_attn.out_proj.weight:                              65536
* encoder.layers.4.self_attn.out_proj.bias:                                  256
* encoder.layers.4.self_attn.linear_pos.weight:                            65536
* encoder.layers.4.feed_forward.0.weight:                                 524288
* encoder.layers.4.feed_forward.0.bias:                                     2048
* encoder.layers.4.feed_forward.3.weight:                                 524288
* encoder.layers.4.feed_forward.3.bias:                                      256
* encoder.layers.4.feed_forward_macaron.0.weight:                         524288
* encoder.layers.4.feed_forward_macaron.0.bias:                             2048
* encoder.layers.4.feed_forward_macaron.3.weight:                         524288
* encoder.layers.4.feed_forward_macaron.3.bias:                              256
* encoder.layers.4.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.4.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.4.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.4.conv_module.depthwise_conv.bias:                          256
* encoder.layers.4.conv_module.norm.weight:                                  256
* encoder.layers.4.conv_module.norm.bias:                                    256
* encoder.layers.4.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.4.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.4.norm_ff_macaron.weight:                                   256
* encoder.layers.4.norm_ff_macaron.bias:                                     256
* encoder.layers.4.norm_ff.weight:                                           256
* encoder.layers.4.norm_ff.bias:                                             256
* encoder.layers.4.norm_mha.weight:                                          256
* encoder.layers.4.norm_mha.bias:                                            256
* encoder.layers.4.norm_conv.weight:                                         256
* encoder.layers.4.norm_conv.bias:                                           256
* encoder.layers.4.norm_final.weight:                                        256
* encoder.layers.4.norm_final.bias:                                          256
* encoder.layers.5.self_attn.pos_bias_u:                                     256
* encoder.layers.5.self_attn.pos_bias_v:                                     256
* encoder.layers.5.self_attn.in_proj.weight:                              196608
* encoder.layers.5.self_attn.in_proj.bias:                                   768
* encoder.layers.5.self_attn.out_proj.weight:                              65536
* encoder.layers.5.self_attn.out_proj.bias:                                  256
* encoder.layers.5.self_attn.linear_pos.weight:                            65536
* encoder.layers.5.feed_forward.0.weight:                                 524288
* encoder.layers.5.feed_forward.0.bias:                                     2048
* encoder.layers.5.feed_forward.3.weight:                                 524288
* encoder.layers.5.feed_forward.3.bias:                                      256
* encoder.layers.5.feed_forward_macaron.0.weight:                         524288
* encoder.layers.5.feed_forward_macaron.0.bias:                             2048
* encoder.layers.5.feed_forward_macaron.3.weight:                         524288
* encoder.layers.5.feed_forward_macaron.3.bias:                              256
* encoder.layers.5.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.5.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.5.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.5.conv_module.depthwise_conv.bias:                          256
* encoder.layers.5.conv_module.norm.weight:                                  256
* encoder.layers.5.conv_module.norm.bias:                                    256
* encoder.layers.5.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.5.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.5.norm_ff_macaron.weight:                                   256
* encoder.layers.5.norm_ff_macaron.bias:                                     256
* encoder.layers.5.norm_ff.weight:                                           256
* encoder.layers.5.norm_ff.bias:                                             256
* encoder.layers.5.norm_mha.weight:                                          256
* encoder.layers.5.norm_mha.bias:                                            256
* encoder.layers.5.norm_conv.weight:                                         256
* encoder.layers.5.norm_conv.bias:                                           256
* encoder.layers.5.norm_final.weight:                                        256
* encoder.layers.5.norm_final.bias:                                          256
* encoder.layers.6.self_attn.pos_bias_u:                                     256
* encoder.layers.6.self_attn.pos_bias_v:                                     256
* encoder.layers.6.self_attn.in_proj.weight:                              196608
* encoder.layers.6.self_attn.in_proj.bias:                                   768
* encoder.layers.6.self_attn.out_proj.weight:                              65536
* encoder.layers.6.self_attn.out_proj.bias:                                  256
* encoder.layers.6.self_attn.linear_pos.weight:                            65536
* encoder.layers.6.feed_forward.0.weight:                                 524288
* encoder.layers.6.feed_forward.0.bias:                                     2048
* encoder.layers.6.feed_forward.3.weight:                                 524288
* encoder.layers.6.feed_forward.3.bias:                                      256
* encoder.layers.6.feed_forward_macaron.0.weight:                         524288
* encoder.layers.6.feed_forward_macaron.0.bias:                             2048
* encoder.layers.6.feed_forward_macaron.3.weight:                         524288
* encoder.layers.6.feed_forward_macaron.3.bias:                              256
* encoder.layers.6.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.6.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.6.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.6.conv_module.depthwise_conv.bias:                          256
* encoder.layers.6.conv_module.norm.weight:                                  256
* encoder.layers.6.conv_module.norm.bias:                                    256
* encoder.layers.6.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.6.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.6.norm_ff_macaron.weight:                                   256
* encoder.layers.6.norm_ff_macaron.bias:                                     256
* encoder.layers.6.norm_ff.weight:                                           256
* encoder.layers.6.norm_ff.bias:                                             256
* encoder.layers.6.norm_mha.weight:                                          256
* encoder.layers.6.norm_mha.bias:                                            256
* encoder.layers.6.norm_conv.weight:                                         256
* encoder.layers.6.norm_conv.bias:                                           256
* encoder.layers.6.norm_final.weight:                                        256
* encoder.layers.6.norm_final.bias:                                          256
* encoder.layers.7.self_attn.pos_bias_u:                                     256
* encoder.layers.7.self_attn.pos_bias_v:                                     256
* encoder.layers.7.self_attn.in_proj.weight:                              196608
* encoder.layers.7.self_attn.in_proj.bias:                                   768
* encoder.layers.7.self_attn.out_proj.weight:                              65536
* encoder.layers.7.self_attn.out_proj.bias:                                  256
* encoder.layers.7.self_attn.linear_pos.weight:                            65536
* encoder.layers.7.feed_forward.0.weight:                                 524288
* encoder.layers.7.feed_forward.0.bias:                                     2048
* encoder.layers.7.feed_forward.3.weight:                                 524288
* encoder.layers.7.feed_forward.3.bias:                                      256
* encoder.layers.7.feed_forward_macaron.0.weight:                         524288
* encoder.layers.7.feed_forward_macaron.0.bias:                             2048
* encoder.layers.7.feed_forward_macaron.3.weight:                         524288
* encoder.layers.7.feed_forward_macaron.3.bias:                              256
* encoder.layers.7.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.7.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.7.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.7.conv_module.depthwise_conv.bias:                          256
* encoder.layers.7.conv_module.norm.weight:                                  256
* encoder.layers.7.conv_module.norm.bias:                                    256
* encoder.layers.7.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.7.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.7.norm_ff_macaron.weight:                                   256
* encoder.layers.7.norm_ff_macaron.bias:                                     256
* encoder.layers.7.norm_ff.weight:                                           256
* encoder.layers.7.norm_ff.bias:                                             256
* encoder.layers.7.norm_mha.weight:                                          256
* encoder.layers.7.norm_mha.bias:                                            256
* encoder.layers.7.norm_conv.weight:                                         256
* encoder.layers.7.norm_conv.bias:                                           256
* encoder.layers.7.norm_final.weight:                                        256
* encoder.layers.7.norm_final.bias:                                          256
* encoder.layers.8.self_attn.pos_bias_u:                                     256
* encoder.layers.8.self_attn.pos_bias_v:                                     256
* encoder.layers.8.self_attn.in_proj.weight:                              196608
* encoder.layers.8.self_attn.in_proj.bias:                                   768
* encoder.layers.8.self_attn.out_proj.weight:                              65536
* encoder.layers.8.self_attn.out_proj.bias:                                  256
* encoder.layers.8.self_attn.linear_pos.weight:                            65536
* encoder.layers.8.feed_forward.0.weight:                                 524288
* encoder.layers.8.feed_forward.0.bias:                                     2048
* encoder.layers.8.feed_forward.3.weight:                                 524288
* encoder.layers.8.feed_forward.3.bias:                                      256
* encoder.layers.8.feed_forward_macaron.0.weight:                         524288
* encoder.layers.8.feed_forward_macaron.0.bias:                             2048
* encoder.layers.8.feed_forward_macaron.3.weight:                         524288
* encoder.layers.8.feed_forward_macaron.3.bias:                              256
* encoder.layers.8.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.8.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.8.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.8.conv_module.depthwise_conv.bias:                          256
* encoder.layers.8.conv_module.norm.weight:                                  256
* encoder.layers.8.conv_module.norm.bias:                                    256
* encoder.layers.8.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.8.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.8.norm_ff_macaron.weight:                                   256
* encoder.layers.8.norm_ff_macaron.bias:                                     256
* encoder.layers.8.norm_ff.weight:                                           256
* encoder.layers.8.norm_ff.bias:                                             256
* encoder.layers.8.norm_mha.weight:                                          256
* encoder.layers.8.norm_mha.bias:                                            256
* encoder.layers.8.norm_conv.weight:                                         256
* encoder.layers.8.norm_conv.bias:                                           256
* encoder.layers.8.norm_final.weight:                                        256
* encoder.layers.8.norm_final.bias:                                          256
* encoder.layers.9.self_attn.pos_bias_u:                                     256
* encoder.layers.9.self_attn.pos_bias_v:                                     256
* encoder.layers.9.self_attn.in_proj.weight:                              196608
* encoder.layers.9.self_attn.in_proj.bias:                                   768
* encoder.layers.9.self_attn.out_proj.weight:                              65536
* encoder.layers.9.self_attn.out_proj.bias:                                  256
* encoder.layers.9.self_attn.linear_pos.weight:                            65536
* encoder.layers.9.feed_forward.0.weight:                                 524288
* encoder.layers.9.feed_forward.0.bias:                                     2048
* encoder.layers.9.feed_forward.3.weight:                                 524288
* encoder.layers.9.feed_forward.3.bias:                                      256
* encoder.layers.9.feed_forward_macaron.0.weight:                         524288
* encoder.layers.9.feed_forward_macaron.0.bias:                             2048
* encoder.layers.9.feed_forward_macaron.3.weight:                         524288
* encoder.layers.9.feed_forward_macaron.3.bias:                              256
* encoder.layers.9.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.9.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.9.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.9.conv_module.depthwise_conv.bias:                          256
* encoder.layers.9.conv_module.norm.weight:                                  256
* encoder.layers.9.conv_module.norm.bias:                                    256
* encoder.layers.9.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.9.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.9.norm_ff_macaron.weight:                                   256
* encoder.layers.9.norm_ff_macaron.bias:                                     256
* encoder.layers.9.norm_ff.weight:                                           256
* encoder.layers.9.norm_ff.bias:                                             256
* encoder.layers.9.norm_mha.weight:                                          256
* encoder.layers.9.norm_mha.bias:                                            256
* encoder.layers.9.norm_conv.weight:                                         256
* encoder.layers.9.norm_conv.bias:                                           256
* encoder.layers.9.norm_final.weight:                                        256
* encoder.layers.9.norm_final.bias:                                          256
* encoder.layers.10.self_attn.pos_bias_u:                                    256
* encoder.layers.10.self_attn.pos_bias_v:                                    256
* encoder.layers.10.self_attn.in_proj.weight:                             196608
* encoder.layers.10.self_attn.in_proj.bias:                                  768
* encoder.layers.10.self_attn.out_proj.weight:                             65536
* encoder.layers.10.self_attn.out_proj.bias:                                 256
* encoder.layers.10.self_attn.linear_pos.weight:                           65536
* encoder.layers.10.feed_forward.0.weight:                                524288
* encoder.layers.10.feed_forward.0.bias:                                    2048
* encoder.layers.10.feed_forward.3.weight:                                524288
* encoder.layers.10.feed_forward.3.bias:                                     256
* encoder.layers.10.feed_forward_macaron.0.weight:                        524288
* encoder.layers.10.feed_forward_macaron.0.bias:                            2048
* encoder.layers.10.feed_forward_macaron.3.weight:                        524288
* encoder.layers.10.feed_forward_macaron.3.bias:                             256
* encoder.layers.10.conv_module.pointwise_conv1.weight:                   131072
* encoder.layers.10.conv_module.pointwise_conv1.bias:                        512
* encoder.layers.10.conv_module.depthwise_conv.weight:                      7936
* encoder.layers.10.conv_module.depthwise_conv.bias:                         256
* encoder.layers.10.conv_module.norm.weight:                                 256
* encoder.layers.10.conv_module.norm.bias:                                   256
* encoder.layers.10.conv_module.pointwise_conv2.weight:                    65536
* encoder.layers.10.conv_module.pointwise_conv2.bias:                        256
* encoder.layers.10.norm_ff_macaron.weight:                                  256
* encoder.layers.10.norm_ff_macaron.bias:                                    256
* encoder.layers.10.norm_ff.weight:                                          256
* encoder.layers.10.norm_ff.bias:                                            256
* encoder.layers.10.norm_mha.weight:                                         256
* encoder.layers.10.norm_mha.bias:                                           256
* encoder.layers.10.norm_conv.weight:                                        256
* encoder.layers.10.norm_conv.bias:                                          256
* encoder.layers.10.norm_final.weight:                                       256
* encoder.layers.10.norm_final.bias:                                         256
* encoder.layers.11.self_attn.pos_bias_u:                                    256
* encoder.layers.11.self_attn.pos_bias_v:                                    256
* encoder.layers.11.self_attn.in_proj.weight:                             196608
* encoder.layers.11.self_attn.in_proj.bias:                                  768
* encoder.layers.11.self_attn.out_proj.weight:                             65536
* encoder.layers.11.self_attn.out_proj.bias:                                 256
* encoder.layers.11.self_attn.linear_pos.weight:                           65536
* encoder.layers.11.feed_forward.0.weight:                                524288
* encoder.layers.11.feed_forward.0.bias:                                    2048
* encoder.layers.11.feed_forward.3.weight:                                524288
* encoder.layers.11.feed_forward.3.bias:                                     256
* encoder.layers.11.feed_forward_macaron.0.weight:                        524288
* encoder.layers.11.feed_forward_macaron.0.bias:                            2048
* encoder.layers.11.feed_forward_macaron.3.weight:                        524288
* encoder.layers.11.feed_forward_macaron.3.bias:                             256
* encoder.layers.11.conv_module.pointwise_conv1.weight:                   131072
* encoder.layers.11.conv_module.pointwise_conv1.bias:                        512
* encoder.layers.11.conv_module.depthwise_conv.weight:                      7936
* encoder.layers.11.conv_module.depthwise_conv.bias:                         256
* encoder.layers.11.conv_module.norm.weight:                                 256
* encoder.layers.11.conv_module.norm.bias:                                   256
* encoder.layers.11.conv_module.pointwise_conv2.weight:                    65536
* encoder.layers.11.conv_module.pointwise_conv2.bias:                        256
* encoder.layers.11.norm_ff_macaron.weight:                                  256
* encoder.layers.11.norm_ff_macaron.bias:                                    256
* encoder.layers.11.norm_ff.weight:                                          256
* encoder.layers.11.norm_ff.bias:                                            256
* encoder.layers.11.norm_mha.weight:                                         256
* encoder.layers.11.norm_mha.bias:                                           256
* encoder.layers.11.norm_conv.weight:                                        256
* encoder.layers.11.norm_conv.bias:                                          256
* encoder.layers.11.norm_final.weight:                                       256
* encoder.layers.11.norm_final.bias:                                         256
* encoder_output_layer.1.weight:                                           22272
* encoder_output_layer.1.bias:                                                87
================================================================================
Total: 32081863
================================================================================
No ali_model
epoch 0, learning rate 0
batch 0, epoch 0/10 global average objf: 1.736792 over 12257.0 frames (99.9% kept), current batch average objf: 1.736792 over 12257 frames (99.9% kept) avg time waiting for batch 44.922s
Reducer buckets have been rebuilt in this iteration.
Traceback (most recent call last):
  File "mmi_att_transformer_train.py", line 664, in <module>
    main()
  File "mmi_att_transformer_train.py", line 657, in main
    mp.spawn(run, args=(world_size, args), nprocs=world_size, join=True)
  File "/home/hltcoe/aarora/miniconda3/envs/k2/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/hltcoe/aarora/miniconda3/envs/k2/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/home/hltcoe/aarora/miniconda3/envs/k2/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 150, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/hltcoe/aarora/miniconda3/envs/k2/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/home/hltcoe/aarora/snowfall/egs/safet/asr/simple_v1/mmi_att_transformer_train.py", line 575, in run
    objf, valid_objf, global_batch_idx_train = train_one_epoch(
  File "/home/hltcoe/aarora/snowfall/egs/safet/asr/simple_v1/mmi_att_transformer_train.py", line 245, in train_one_epoch
    curr_batch_objf, curr_batch_frames, curr_batch_all_frames = get_objf(
  File "/home/hltcoe/aarora/snowfall/egs/safet/asr/simple_v1/mmi_att_transformer_train.py", line 84, in get_objf
    nnet_output, encoder_memory, memory_mask = model(feature, supervisions)
  File "/home/hltcoe/aarora/miniconda3/envs/k2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/hltcoe/aarora/miniconda3/envs/k2/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 705, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/hltcoe/aarora/miniconda3/envs/k2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/hltcoe/aarora/snowfall/snowfall/models/transformer.py", line 95, in forward
    encoder_memory, memory_mask = self.encode(x, supervision)
  File "/home/hltcoe/aarora/snowfall/snowfall/models/conformer.py", line 67, in encode
    x = self.encoder(x, pos_emb, src_key_padding_mask=mask)  # (T, B, F)
  File "/home/hltcoe/aarora/miniconda3/envs/k2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/hltcoe/aarora/snowfall/snowfall/models/conformer.py", line 224, in forward
    output = mod(output, pos_emb, src_mask=mask, src_key_padding_mask=src_key_padding_mask)
  File "/home/hltcoe/aarora/miniconda3/envs/k2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/hltcoe/aarora/snowfall/snowfall/models/conformer.py", line 157, in forward
    src_att = self.self_attn(src, src, src, pos_emb=pos_emb, attn_mask=src_mask,
  File "/home/hltcoe/aarora/miniconda3/envs/k2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/hltcoe/aarora/snowfall/snowfall/models/conformer.py", line 393, in forward
    return self.multi_head_attention_forward(
  File "/home/hltcoe/aarora/snowfall/snowfall/models/conformer.py", line 631, in multi_head_attention_forward
    return attn_output, attn_output_weights.sum(dim=1) / num_heads
RuntimeError: CUDA out of memory. Tried to allocate 440.00 MiB (GPU 0; 31.75 GiB total capacity; 29.29 GiB already allocated; 275.50 MiB free; 30.26 GiB reserved in total by PyTorch)

/cm/local/apps/uge/var/spool/r7n04/job_scripts/9177364: ended at Wed May 19 12:51:04 EDT 2021
