# Running on r7n04
# Started at Wed Jun 2 10:35:19 EDT 2021
# /home/hltcoe/aarora/miniconda3/envs/k2/bin/python3 mmi_att_transformer_train.py 
Added key: store_based_barrier_key:1 to store for rank: 0
Loading lexicon and symbol tables
Loading L.fst
About to get train cuts
About to get train cuts
About to get Musan cuts
About to create train dataset
Using cut concatenation with duration factor 1.0 and gap 1.0.
Using SingleCutSampler.
About to create train dataloader
About to get dev cuts
About to get valid cuts
About to create dev dataset
About to create dev dataloader
Use pruned intersect for den_lats
About to create model
================================================================================
Model parameters summary:
================================================================================
* P_scores:                                                                27888
* encoder_embed.layers.0.weight:                                             288
* encoder_embed.layers.0.bias:                                                32
* encoder_embed.layers.2.weight:                                            9216
* encoder_embed.layers.2.bias:                                                32
* encoder_embed.layers.4.weight:                                           18432
* encoder_embed.layers.4.bias:                                                64
* encoder_embed.layers.6.weight:                                           36864
* encoder_embed.layers.6.bias:                                                64
* encoder_embed.out.weight:                                               311296
* encoder_embed.out.bias:                                                    256
* encoder.layers.0.self_attn.pos_bias_u:                                     256
* encoder.layers.0.self_attn.pos_bias_v:                                     256
* encoder.layers.0.self_attn.in_proj.weight:                              196608
* encoder.layers.0.self_attn.in_proj.bias:                                   768
* encoder.layers.0.self_attn.out_proj.weight:                              65536
* encoder.layers.0.self_attn.out_proj.bias:                                  256
* encoder.layers.0.self_attn.linear_pos.weight:                            65536
* encoder.layers.0.feed_forward.0.weight:                                 524288
* encoder.layers.0.feed_forward.0.bias:                                     2048
* encoder.layers.0.feed_forward.3.weight:                                 524288
* encoder.layers.0.feed_forward.3.bias:                                      256
* encoder.layers.0.feed_forward_macaron.0.weight:                         524288
* encoder.layers.0.feed_forward_macaron.0.bias:                             2048
* encoder.layers.0.feed_forward_macaron.3.weight:                         524288
* encoder.layers.0.feed_forward_macaron.3.bias:                              256
* encoder.layers.0.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.0.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.0.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.0.conv_module.depthwise_conv.bias:                          256
* encoder.layers.0.conv_module.norm.weight:                                  256
* encoder.layers.0.conv_module.norm.bias:                                    256
* encoder.layers.0.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.0.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.0.norm_ff_macaron.weight:                                   256
* encoder.layers.0.norm_ff_macaron.bias:                                     256
* encoder.layers.0.norm_ff.weight:                                           256
* encoder.layers.0.norm_ff.bias:                                             256
* encoder.layers.0.norm_mha.weight:                                          256
* encoder.layers.0.norm_mha.bias:                                            256
* encoder.layers.0.norm_conv.weight:                                         256
* encoder.layers.0.norm_conv.bias:                                           256
* encoder.layers.0.norm_final.weight:                                        256
* encoder.layers.0.norm_final.bias:                                          256
* encoder.layers.1.self_attn.pos_bias_u:                                     256
* encoder.layers.1.self_attn.pos_bias_v:                                     256
* encoder.layers.1.self_attn.in_proj.weight:                              196608
* encoder.layers.1.self_attn.in_proj.bias:                                   768
* encoder.layers.1.self_attn.out_proj.weight:                              65536
* encoder.layers.1.self_attn.out_proj.bias:                                  256
* encoder.layers.1.self_attn.linear_pos.weight:                            65536
* encoder.layers.1.feed_forward.0.weight:                                 524288
* encoder.layers.1.feed_forward.0.bias:                                     2048
* encoder.layers.1.feed_forward.3.weight:                                 524288
* encoder.layers.1.feed_forward.3.bias:                                      256
* encoder.layers.1.feed_forward_macaron.0.weight:                         524288
* encoder.layers.1.feed_forward_macaron.0.bias:                             2048
* encoder.layers.1.feed_forward_macaron.3.weight:                         524288
* encoder.layers.1.feed_forward_macaron.3.bias:                              256
* encoder.layers.1.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.1.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.1.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.1.conv_module.depthwise_conv.bias:                          256
* encoder.layers.1.conv_module.norm.weight:                                  256
* encoder.layers.1.conv_module.norm.bias:                                    256
* encoder.layers.1.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.1.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.1.norm_ff_macaron.weight:                                   256
* encoder.layers.1.norm_ff_macaron.bias:                                     256
* encoder.layers.1.norm_ff.weight:                                           256
* encoder.layers.1.norm_ff.bias:                                             256
* encoder.layers.1.norm_mha.weight:                                          256
* encoder.layers.1.norm_mha.bias:                                            256
* encoder.layers.1.norm_conv.weight:                                         256
* encoder.layers.1.norm_conv.bias:                                           256
* encoder.layers.1.norm_final.weight:                                        256
* encoder.layers.1.norm_final.bias:                                          256
* encoder.layers.2.self_attn.pos_bias_u:                                     256
* encoder.layers.2.self_attn.pos_bias_v:                                     256
* encoder.layers.2.self_attn.in_proj.weight:                              196608
* encoder.layers.2.self_attn.in_proj.bias:                                   768
* encoder.layers.2.self_attn.out_proj.weight:                              65536
* encoder.layers.2.self_attn.out_proj.bias:                                  256
* encoder.layers.2.self_attn.linear_pos.weight:                            65536
* encoder.layers.2.feed_forward.0.weight:                                 524288
* encoder.layers.2.feed_forward.0.bias:                                     2048
* encoder.layers.2.feed_forward.3.weight:                                 524288
* encoder.layers.2.feed_forward.3.bias:                                      256
* encoder.layers.2.feed_forward_macaron.0.weight:                         524288
* encoder.layers.2.feed_forward_macaron.0.bias:                             2048
* encoder.layers.2.feed_forward_macaron.3.weight:                         524288
* encoder.layers.2.feed_forward_macaron.3.bias:                              256
* encoder.layers.2.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.2.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.2.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.2.conv_module.depthwise_conv.bias:                          256
* encoder.layers.2.conv_module.norm.weight:                                  256
* encoder.layers.2.conv_module.norm.bias:                                    256
* encoder.layers.2.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.2.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.2.norm_ff_macaron.weight:                                   256
* encoder.layers.2.norm_ff_macaron.bias:                                     256
* encoder.layers.2.norm_ff.weight:                                           256
* encoder.layers.2.norm_ff.bias:                                             256
* encoder.layers.2.norm_mha.weight:                                          256
* encoder.layers.2.norm_mha.bias:                                            256
* encoder.layers.2.norm_conv.weight:                                         256
* encoder.layers.2.norm_conv.bias:                                           256
* encoder.layers.2.norm_final.weight:                                        256
* encoder.layers.2.norm_final.bias:                                          256
* encoder.layers.3.self_attn.pos_bias_u:                                     256
* encoder.layers.3.self_attn.pos_bias_v:                                     256
* encoder.layers.3.self_attn.in_proj.weight:                              196608
* encoder.layers.3.self_attn.in_proj.bias:                                   768
* encoder.layers.3.self_attn.out_proj.weight:                              65536
* encoder.layers.3.self_attn.out_proj.bias:                                  256
* encoder.layers.3.self_attn.linear_pos.weight:                            65536
* encoder.layers.3.feed_forward.0.weight:                                 524288
* encoder.layers.3.feed_forward.0.bias:                                     2048
* encoder.layers.3.feed_forward.3.weight:                                 524288
* encoder.layers.3.feed_forward.3.bias:                                      256
* encoder.layers.3.feed_forward_macaron.0.weight:                         524288
* encoder.layers.3.feed_forward_macaron.0.bias:                             2048
* encoder.layers.3.feed_forward_macaron.3.weight:                         524288
* encoder.layers.3.feed_forward_macaron.3.bias:                              256
* encoder.layers.3.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.3.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.3.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.3.conv_module.depthwise_conv.bias:                          256
* encoder.layers.3.conv_module.norm.weight:                                  256
* encoder.layers.3.conv_module.norm.bias:                                    256
* encoder.layers.3.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.3.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.3.norm_ff_macaron.weight:                                   256
* encoder.layers.3.norm_ff_macaron.bias:                                     256
* encoder.layers.3.norm_ff.weight:                                           256
* encoder.layers.3.norm_ff.bias:                                             256
* encoder.layers.3.norm_mha.weight:                                          256
* encoder.layers.3.norm_mha.bias:                                            256
* encoder.layers.3.norm_conv.weight:                                         256
* encoder.layers.3.norm_conv.bias:                                           256
* encoder.layers.3.norm_final.weight:                                        256
* encoder.layers.3.norm_final.bias:                                          256
* encoder.layers.4.self_attn.pos_bias_u:                                     256
* encoder.layers.4.self_attn.pos_bias_v:                                     256
* encoder.layers.4.self_attn.in_proj.weight:                              196608
* encoder.layers.4.self_attn.in_proj.bias:                                   768
* encoder.layers.4.self_attn.out_proj.weight:                              65536
* encoder.layers.4.self_attn.out_proj.bias:                                  256
* encoder.layers.4.self_attn.linear_pos.weight:                            65536
* encoder.layers.4.feed_forward.0.weight:                                 524288
* encoder.layers.4.feed_forward.0.bias:                                     2048
* encoder.layers.4.feed_forward.3.weight:                                 524288
* encoder.layers.4.feed_forward.3.bias:                                      256
* encoder.layers.4.feed_forward_macaron.0.weight:                         524288
* encoder.layers.4.feed_forward_macaron.0.bias:                             2048
* encoder.layers.4.feed_forward_macaron.3.weight:                         524288
* encoder.layers.4.feed_forward_macaron.3.bias:                              256
* encoder.layers.4.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.4.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.4.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.4.conv_module.depthwise_conv.bias:                          256
* encoder.layers.4.conv_module.norm.weight:                                  256
* encoder.layers.4.conv_module.norm.bias:                                    256
* encoder.layers.4.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.4.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.4.norm_ff_macaron.weight:                                   256
* encoder.layers.4.norm_ff_macaron.bias:                                     256
* encoder.layers.4.norm_ff.weight:                                           256
* encoder.layers.4.norm_ff.bias:                                             256
* encoder.layers.4.norm_mha.weight:                                          256
* encoder.layers.4.norm_mha.bias:                                            256
* encoder.layers.4.norm_conv.weight:                                         256
* encoder.layers.4.norm_conv.bias:                                           256
* encoder.layers.4.norm_final.weight:                                        256
* encoder.layers.4.norm_final.bias:                                          256
* encoder.layers.5.self_attn.pos_bias_u:                                     256
* encoder.layers.5.self_attn.pos_bias_v:                                     256
* encoder.layers.5.self_attn.in_proj.weight:                              196608
* encoder.layers.5.self_attn.in_proj.bias:                                   768
* encoder.layers.5.self_attn.out_proj.weight:                              65536
* encoder.layers.5.self_attn.out_proj.bias:                                  256
* encoder.layers.5.self_attn.linear_pos.weight:                            65536
* encoder.layers.5.feed_forward.0.weight:                                 524288
* encoder.layers.5.feed_forward.0.bias:                                     2048
* encoder.layers.5.feed_forward.3.weight:                                 524288
* encoder.layers.5.feed_forward.3.bias:                                      256
* encoder.layers.5.feed_forward_macaron.0.weight:                         524288
* encoder.layers.5.feed_forward_macaron.0.bias:                             2048
* encoder.layers.5.feed_forward_macaron.3.weight:                         524288
* encoder.layers.5.feed_forward_macaron.3.bias:                              256
* encoder.layers.5.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.5.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.5.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.5.conv_module.depthwise_conv.bias:                          256
* encoder.layers.5.conv_module.norm.weight:                                  256
* encoder.layers.5.conv_module.norm.bias:                                    256
* encoder.layers.5.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.5.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.5.norm_ff_macaron.weight:                                   256
* encoder.layers.5.norm_ff_macaron.bias:                                     256
* encoder.layers.5.norm_ff.weight:                                           256
* encoder.layers.5.norm_ff.bias:                                             256
* encoder.layers.5.norm_mha.weight:                                          256
* encoder.layers.5.norm_mha.bias:                                            256
* encoder.layers.5.norm_conv.weight:                                         256
* encoder.layers.5.norm_conv.bias:                                           256
* encoder.layers.5.norm_final.weight:                                        256
* encoder.layers.5.norm_final.bias:                                          256
* encoder.layers.6.self_attn.pos_bias_u:                                     256
* encoder.layers.6.self_attn.pos_bias_v:                                     256
* encoder.layers.6.self_attn.in_proj.weight:                              196608
* encoder.layers.6.self_attn.in_proj.bias:                                   768
* encoder.layers.6.self_attn.out_proj.weight:                              65536
* encoder.layers.6.self_attn.out_proj.bias:                                  256
* encoder.layers.6.self_attn.linear_pos.weight:                            65536
* encoder.layers.6.feed_forward.0.weight:                                 524288
* encoder.layers.6.feed_forward.0.bias:                                     2048
* encoder.layers.6.feed_forward.3.weight:                                 524288
* encoder.layers.6.feed_forward.3.bias:                                      256
* encoder.layers.6.feed_forward_macaron.0.weight:                         524288
* encoder.layers.6.feed_forward_macaron.0.bias:                             2048
* encoder.layers.6.feed_forward_macaron.3.weight:                         524288
* encoder.layers.6.feed_forward_macaron.3.bias:                              256
* encoder.layers.6.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.6.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.6.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.6.conv_module.depthwise_conv.bias:                          256
* encoder.layers.6.conv_module.norm.weight:                                  256
* encoder.layers.6.conv_module.norm.bias:                                    256
* encoder.layers.6.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.6.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.6.norm_ff_macaron.weight:                                   256
* encoder.layers.6.norm_ff_macaron.bias:                                     256
* encoder.layers.6.norm_ff.weight:                                           256
* encoder.layers.6.norm_ff.bias:                                             256
* encoder.layers.6.norm_mha.weight:                                          256
* encoder.layers.6.norm_mha.bias:                                            256
* encoder.layers.6.norm_conv.weight:                                         256
* encoder.layers.6.norm_conv.bias:                                           256
* encoder.layers.6.norm_final.weight:                                        256
* encoder.layers.6.norm_final.bias:                                          256
* encoder.layers.7.self_attn.pos_bias_u:                                     256
* encoder.layers.7.self_attn.pos_bias_v:                                     256
* encoder.layers.7.self_attn.in_proj.weight:                              196608
* encoder.layers.7.self_attn.in_proj.bias:                                   768
* encoder.layers.7.self_attn.out_proj.weight:                              65536
* encoder.layers.7.self_attn.out_proj.bias:                                  256
* encoder.layers.7.self_attn.linear_pos.weight:                            65536
* encoder.layers.7.feed_forward.0.weight:                                 524288
* encoder.layers.7.feed_forward.0.bias:                                     2048
* encoder.layers.7.feed_forward.3.weight:                                 524288
* encoder.layers.7.feed_forward.3.bias:                                      256
* encoder.layers.7.feed_forward_macaron.0.weight:                         524288
* encoder.layers.7.feed_forward_macaron.0.bias:                             2048
* encoder.layers.7.feed_forward_macaron.3.weight:                         524288
* encoder.layers.7.feed_forward_macaron.3.bias:                              256
* encoder.layers.7.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.7.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.7.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.7.conv_module.depthwise_conv.bias:                          256
* encoder.layers.7.conv_module.norm.weight:                                  256
* encoder.layers.7.conv_module.norm.bias:                                    256
* encoder.layers.7.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.7.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.7.norm_ff_macaron.weight:                                   256
* encoder.layers.7.norm_ff_macaron.bias:                                     256
* encoder.layers.7.norm_ff.weight:                                           256
* encoder.layers.7.norm_ff.bias:                                             256
* encoder.layers.7.norm_mha.weight:                                          256
* encoder.layers.7.norm_mha.bias:                                            256
* encoder.layers.7.norm_conv.weight:                                         256
* encoder.layers.7.norm_conv.bias:                                           256
* encoder.layers.7.norm_final.weight:                                        256
* encoder.layers.7.norm_final.bias:                                          256
* encoder.layers.8.self_attn.pos_bias_u:                                     256
* encoder.layers.8.self_attn.pos_bias_v:                                     256
* encoder.layers.8.self_attn.in_proj.weight:                              196608
* encoder.layers.8.self_attn.in_proj.bias:                                   768
* encoder.layers.8.self_attn.out_proj.weight:                              65536
* encoder.layers.8.self_attn.out_proj.bias:                                  256
* encoder.layers.8.self_attn.linear_pos.weight:                            65536
* encoder.layers.8.feed_forward.0.weight:                                 524288
* encoder.layers.8.feed_forward.0.bias:                                     2048
* encoder.layers.8.feed_forward.3.weight:                                 524288
* encoder.layers.8.feed_forward.3.bias:                                      256
* encoder.layers.8.feed_forward_macaron.0.weight:                         524288
* encoder.layers.8.feed_forward_macaron.0.bias:                             2048
* encoder.layers.8.feed_forward_macaron.3.weight:                         524288
* encoder.layers.8.feed_forward_macaron.3.bias:                              256
* encoder.layers.8.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.8.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.8.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.8.conv_module.depthwise_conv.bias:                          256
* encoder.layers.8.conv_module.norm.weight:                                  256
* encoder.layers.8.conv_module.norm.bias:                                    256
* encoder.layers.8.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.8.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.8.norm_ff_macaron.weight:                                   256
* encoder.layers.8.norm_ff_macaron.bias:                                     256
* encoder.layers.8.norm_ff.weight:                                           256
* encoder.layers.8.norm_ff.bias:                                             256
* encoder.layers.8.norm_mha.weight:                                          256
* encoder.layers.8.norm_mha.bias:                                            256
* encoder.layers.8.norm_conv.weight:                                         256
* encoder.layers.8.norm_conv.bias:                                           256
* encoder.layers.8.norm_final.weight:                                        256
* encoder.layers.8.norm_final.bias:                                          256
* encoder.layers.9.self_attn.pos_bias_u:                                     256
* encoder.layers.9.self_attn.pos_bias_v:                                     256
* encoder.layers.9.self_attn.in_proj.weight:                              196608
* encoder.layers.9.self_attn.in_proj.bias:                                   768
* encoder.layers.9.self_attn.out_proj.weight:                              65536
* encoder.layers.9.self_attn.out_proj.bias:                                  256
* encoder.layers.9.self_attn.linear_pos.weight:                            65536
* encoder.layers.9.feed_forward.0.weight:                                 524288
* encoder.layers.9.feed_forward.0.bias:                                     2048
* encoder.layers.9.feed_forward.3.weight:                                 524288
* encoder.layers.9.feed_forward.3.bias:                                      256
* encoder.layers.9.feed_forward_macaron.0.weight:                         524288
* encoder.layers.9.feed_forward_macaron.0.bias:                             2048
* encoder.layers.9.feed_forward_macaron.3.weight:                         524288
* encoder.layers.9.feed_forward_macaron.3.bias:                              256
* encoder.layers.9.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.9.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.9.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.9.conv_module.depthwise_conv.bias:                          256
* encoder.layers.9.conv_module.norm.weight:                                  256
* encoder.layers.9.conv_module.norm.bias:                                    256
* encoder.layers.9.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.9.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.9.norm_ff_macaron.weight:                                   256
* encoder.layers.9.norm_ff_macaron.bias:                                     256
* encoder.layers.9.norm_ff.weight:                                           256
* encoder.layers.9.norm_ff.bias:                                             256
* encoder.layers.9.norm_mha.weight:                                          256
* encoder.layers.9.norm_mha.bias:                                            256
* encoder.layers.9.norm_conv.weight:                                         256
* encoder.layers.9.norm_conv.bias:                                           256
* encoder.layers.9.norm_final.weight:                                        256
* encoder.layers.9.norm_final.bias:                                          256
* encoder.layers.10.self_attn.pos_bias_u:                                    256
* encoder.layers.10.self_attn.pos_bias_v:                                    256
* encoder.layers.10.self_attn.in_proj.weight:                             196608
* encoder.layers.10.self_attn.in_proj.bias:                                  768
* encoder.layers.10.self_attn.out_proj.weight:                             65536
* encoder.layers.10.self_attn.out_proj.bias:                                 256
* encoder.layers.10.self_attn.linear_pos.weight:                           65536
* encoder.layers.10.feed_forward.0.weight:                                524288
* encoder.layers.10.feed_forward.0.bias:                                    2048
* encoder.layers.10.feed_forward.3.weight:                                524288
* encoder.layers.10.feed_forward.3.bias:                                     256
* encoder.layers.10.feed_forward_macaron.0.weight:                        524288
* encoder.layers.10.feed_forward_macaron.0.bias:                            2048
* encoder.layers.10.feed_forward_macaron.3.weight:                        524288
* encoder.layers.10.feed_forward_macaron.3.bias:                             256
* encoder.layers.10.conv_module.pointwise_conv1.weight:                   131072
* encoder.layers.10.conv_module.pointwise_conv1.bias:                        512
* encoder.layers.10.conv_module.depthwise_conv.weight:                      7936
* encoder.layers.10.conv_module.depthwise_conv.bias:                         256
* encoder.layers.10.conv_module.norm.weight:                                 256
* encoder.layers.10.conv_module.norm.bias:                                   256
* encoder.layers.10.conv_module.pointwise_conv2.weight:                    65536
* encoder.layers.10.conv_module.pointwise_conv2.bias:                        256
* encoder.layers.10.norm_ff_macaron.weight:                                  256
* encoder.layers.10.norm_ff_macaron.bias:                                    256
* encoder.layers.10.norm_ff.weight:                                          256
* encoder.layers.10.norm_ff.bias:                                            256
* encoder.layers.10.norm_mha.weight:                                         256
* encoder.layers.10.norm_mha.bias:                                           256
* encoder.layers.10.norm_conv.weight:                                        256
* encoder.layers.10.norm_conv.bias:                                          256
* encoder.layers.10.norm_final.weight:                                       256
* encoder.layers.10.norm_final.bias:                                         256
* encoder.layers.11.self_attn.pos_bias_u:                                    256
* encoder.layers.11.self_attn.pos_bias_v:                                    256
* encoder.layers.11.self_attn.in_proj.weight:                             196608
* encoder.layers.11.self_attn.in_proj.bias:                                  768
* encoder.layers.11.self_attn.out_proj.weight:                             65536
* encoder.layers.11.self_attn.out_proj.bias:                                 256
* encoder.layers.11.self_attn.linear_pos.weight:                           65536
* encoder.layers.11.feed_forward.0.weight:                                524288
* encoder.layers.11.feed_forward.0.bias:                                    2048
* encoder.layers.11.feed_forward.3.weight:                                524288
* encoder.layers.11.feed_forward.3.bias:                                     256
* encoder.layers.11.feed_forward_macaron.0.weight:                        524288
* encoder.layers.11.feed_forward_macaron.0.bias:                            2048
* encoder.layers.11.feed_forward_macaron.3.weight:                        524288
* encoder.layers.11.feed_forward_macaron.3.bias:                             256
* encoder.layers.11.conv_module.pointwise_conv1.weight:                   131072
* encoder.layers.11.conv_module.pointwise_conv1.bias:                        512
* encoder.layers.11.conv_module.depthwise_conv.weight:                      7936
* encoder.layers.11.conv_module.depthwise_conv.bias:                         256
* encoder.layers.11.conv_module.norm.weight:                                 256
* encoder.layers.11.conv_module.norm.bias:                                   256
* encoder.layers.11.conv_module.pointwise_conv2.weight:                    65536
* encoder.layers.11.conv_module.pointwise_conv2.bias:                        256
* encoder.layers.11.norm_ff_macaron.weight:                                  256
* encoder.layers.11.norm_ff_macaron.bias:                                    256
* encoder.layers.11.norm_ff.weight:                                          256
* encoder.layers.11.norm_ff.bias:                                            256
* encoder.layers.11.norm_mha.weight:                                         256
* encoder.layers.11.norm_mha.bias:                                           256
* encoder.layers.11.norm_conv.weight:                                        256
* encoder.layers.11.norm_conv.bias:                                          256
* encoder.layers.11.norm_final.weight:                                       256
* encoder.layers.11.norm_final.bias:                                         256
* encoder_output_layer.1.weight:                                           42752
* encoder_output_layer.1.bias:                                               167
================================================================================
Total: 32122743
================================================================================
Traceback (most recent call last):
  File "mmi_att_transformer_train.py", line 701, in <module>
    main()
  File "mmi_att_transformer_train.py", line 694, in main
    mp.spawn(run, args=(world_size, args), nprocs=world_size, join=True)
  File "/home/hltcoe/aarora/miniconda3/envs/k2/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/hltcoe/aarora/miniconda3/envs/k2/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/home/hltcoe/aarora/miniconda3/envs/k2/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 150, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/hltcoe/aarora/miniconda3/envs/k2/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/home/hltcoe/aarora/snowfall/egs/safet/asr/simple_v1/mmi_att_transformer_train.py", line 569, in run
    ali_model.load_state_dict(torch.load(ali_model_fname, map_location='cpu')['state_dict'])
  File "/home/hltcoe/aarora/miniconda3/envs/k2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1223, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for ContextNet:
	Unexpected key(s) in state_dict: "P_scores". 

# Accounting: time=18 threads=1
# Finished at Wed Jun 2 10:35:37 EDT 2021 with status 1
