SGE_HGR_gpu=5
CUDA_VISIBLE_DEVICES=5
Wed May 19 20:10:47 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100S-PCI...  On   | 00000000:1A:00.0 Off |                    0 |
| N/A   54C    P0   196W / 200W |  10695MiB / 32510MiB |     99%   E. Process |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100S-PCI...  On   | 00000000:1B:00.0 Off |                    0 |
| N/A   55C    P0   144W / 200W |  10695MiB / 32510MiB |     99%   E. Process |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100S-PCI...  On   | 00000000:1C:00.0 Off |                    0 |
| N/A   27C    P0    25W / 200W |      0MiB / 32510MiB |      0%   E. Process |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100S-PCI...  On   | 00000000:3D:00.0 Off |                    0 |
| N/A   62C    P0   183W / 200W |  31097MiB / 32510MiB |    100%   E. Process |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   4  Tesla V100S-PCI...  On   | 00000000:3E:00.0 Off |                    0 |
| N/A   54C    P0   177W / 200W |  10695MiB / 32510MiB |     97%   E. Process |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   5  Tesla V100S-PCI...  On   | 00000000:8B:00.0 Off |                    0 |
| N/A   29C    P0    25W / 200W |      0MiB / 32510MiB |      0%   E. Process |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   6  Tesla V100S-PCI...  On   | 00000000:8C:00.0 Off |                    0 |
| N/A   58C    P0   189W / 200W |  31097MiB / 32510MiB |    100%   E. Process |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   7  Tesla V100S-PCI...  On   | 00000000:B4:00.0 Off |                    0 |
| N/A   45C    P0    71W / 200W |  31097MiB / 32510MiB |    100%   E. Process |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   8  Tesla V100S-PCI...  On   | 00000000:B5:00.0 Off |                    0 |
| N/A   27C    P0    25W / 200W |      0MiB / 32510MiB |      0%   E. Process |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   9  Tesla V100S-PCI...  On   | 00000000:B6:00.0 Off |                    0 |
| N/A   26C    P0    24W / 200W |      0MiB / 32510MiB |      0%   E. Process |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A    361736      C   python                          10691MiB |
|    1   N/A  N/A    361737      C   python                          10691MiB |
|    3   N/A  N/A    277943      C   python                          31093MiB |
|    4   N/A  N/A    361738      C   python                          10691MiB |
|    6   N/A  N/A    130876      C   python                          31093MiB |
|    7   N/A  N/A    100732      C   python                          31093MiB |
+-----------------------------------------------------------------------------+
/cm/local/apps/uge/var/spool/r8n04/job_scripts/9177643: Running on r8n04
/cm/local/apps/uge/var/spool/r8n04/job_scripts/9177643: Started at Wed May 19 20:10:48 EDT 2021
/cm/local/apps/uge/var/spool/r8n04/job_scripts/9177643: Running the job on GPU(s) 5
Added key: store_based_barrier_key:1 to store for rank: 0
Loading lexicon and symbol tables
Loading L.fst
About to get train cuts
About to get train cuts
About to get Musan cuts
About to create train dataset
Using cut concatenation with duration factor 1.0 and gap 1.0.
Using SingleCutSampler.
About to create train dataloader
About to get dev cuts
About to get valid cuts
About to create dev dataset
About to create dev dataloader
About to create model
================================================================================
Model parameters summary:
================================================================================
* P_scores:                                                                 7568
* encoder_embed.layers.0.weight:                                             288
* encoder_embed.layers.0.bias:                                                32
* encoder_embed.layers.2.weight:                                            9216
* encoder_embed.layers.2.bias:                                                32
* encoder_embed.layers.4.weight:                                           18432
* encoder_embed.layers.4.bias:                                                64
* encoder_embed.layers.6.weight:                                           36864
* encoder_embed.layers.6.bias:                                                64
* encoder_embed.out.weight:                                               311296
* encoder_embed.out.bias:                                                    256
* encoder.layers.0.self_attn.pos_bias_u:                                     256
* encoder.layers.0.self_attn.pos_bias_v:                                     256
* encoder.layers.0.self_attn.in_proj.weight:                              196608
* encoder.layers.0.self_attn.in_proj.bias:                                   768
* encoder.layers.0.self_attn.out_proj.weight:                              65536
* encoder.layers.0.self_attn.out_proj.bias:                                  256
* encoder.layers.0.self_attn.linear_pos.weight:                            65536
* encoder.layers.0.feed_forward.0.weight:                                 524288
* encoder.layers.0.feed_forward.0.bias:                                     2048
* encoder.layers.0.feed_forward.3.weight:                                 524288
* encoder.layers.0.feed_forward.3.bias:                                      256
* encoder.layers.0.feed_forward_macaron.0.weight:                         524288
* encoder.layers.0.feed_forward_macaron.0.bias:                             2048
* encoder.layers.0.feed_forward_macaron.3.weight:                         524288
* encoder.layers.0.feed_forward_macaron.3.bias:                              256
* encoder.layers.0.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.0.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.0.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.0.conv_module.depthwise_conv.bias:                          256
* encoder.layers.0.conv_module.norm.weight:                                  256
* encoder.layers.0.conv_module.norm.bias:                                    256
* encoder.layers.0.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.0.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.0.norm_ff_macaron.weight:                                   256
* encoder.layers.0.norm_ff_macaron.bias:                                     256
* encoder.layers.0.norm_ff.weight:                                           256
* encoder.layers.0.norm_ff.bias:                                             256
* encoder.layers.0.norm_mha.weight:                                          256
* encoder.layers.0.norm_mha.bias:                                            256
* encoder.layers.0.norm_conv.weight:                                         256
* encoder.layers.0.norm_conv.bias:                                           256
* encoder.layers.0.norm_final.weight:                                        256
* encoder.layers.0.norm_final.bias:                                          256
* encoder.layers.1.self_attn.pos_bias_u:                                     256
* encoder.layers.1.self_attn.pos_bias_v:                                     256
* encoder.layers.1.self_attn.in_proj.weight:                              196608
* encoder.layers.1.self_attn.in_proj.bias:                                   768
* encoder.layers.1.self_attn.out_proj.weight:                              65536
* encoder.layers.1.self_attn.out_proj.bias:                                  256
* encoder.layers.1.self_attn.linear_pos.weight:                            65536
* encoder.layers.1.feed_forward.0.weight:                                 524288
* encoder.layers.1.feed_forward.0.bias:                                     2048
* encoder.layers.1.feed_forward.3.weight:                                 524288
* encoder.layers.1.feed_forward.3.bias:                                      256
* encoder.layers.1.feed_forward_macaron.0.weight:                         524288
* encoder.layers.1.feed_forward_macaron.0.bias:                             2048
* encoder.layers.1.feed_forward_macaron.3.weight:                         524288
* encoder.layers.1.feed_forward_macaron.3.bias:                              256
* encoder.layers.1.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.1.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.1.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.1.conv_module.depthwise_conv.bias:                          256
* encoder.layers.1.conv_module.norm.weight:                                  256
* encoder.layers.1.conv_module.norm.bias:                                    256
* encoder.layers.1.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.1.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.1.norm_ff_macaron.weight:                                   256
* encoder.layers.1.norm_ff_macaron.bias:                                     256
* encoder.layers.1.norm_ff.weight:                                           256
* encoder.layers.1.norm_ff.bias:                                             256
* encoder.layers.1.norm_mha.weight:                                          256
* encoder.layers.1.norm_mha.bias:                                            256
* encoder.layers.1.norm_conv.weight:                                         256
* encoder.layers.1.norm_conv.bias:                                           256
* encoder.layers.1.norm_final.weight:                                        256
* encoder.layers.1.norm_final.bias:                                          256
* encoder.layers.2.self_attn.pos_bias_u:                                     256
* encoder.layers.2.self_attn.pos_bias_v:                                     256
* encoder.layers.2.self_attn.in_proj.weight:                              196608
* encoder.layers.2.self_attn.in_proj.bias:                                   768
* encoder.layers.2.self_attn.out_proj.weight:                              65536
* encoder.layers.2.self_attn.out_proj.bias:                                  256
* encoder.layers.2.self_attn.linear_pos.weight:                            65536
* encoder.layers.2.feed_forward.0.weight:                                 524288
* encoder.layers.2.feed_forward.0.bias:                                     2048
* encoder.layers.2.feed_forward.3.weight:                                 524288
* encoder.layers.2.feed_forward.3.bias:                                      256
* encoder.layers.2.feed_forward_macaron.0.weight:                         524288
* encoder.layers.2.feed_forward_macaron.0.bias:                             2048
* encoder.layers.2.feed_forward_macaron.3.weight:                         524288
* encoder.layers.2.feed_forward_macaron.3.bias:                              256
* encoder.layers.2.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.2.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.2.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.2.conv_module.depthwise_conv.bias:                          256
* encoder.layers.2.conv_module.norm.weight:                                  256
* encoder.layers.2.conv_module.norm.bias:                                    256
* encoder.layers.2.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.2.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.2.norm_ff_macaron.weight:                                   256
* encoder.layers.2.norm_ff_macaron.bias:                                     256
* encoder.layers.2.norm_ff.weight:                                           256
* encoder.layers.2.norm_ff.bias:                                             256
* encoder.layers.2.norm_mha.weight:                                          256
* encoder.layers.2.norm_mha.bias:                                            256
* encoder.layers.2.norm_conv.weight:                                         256
* encoder.layers.2.norm_conv.bias:                                           256
* encoder.layers.2.norm_final.weight:                                        256
* encoder.layers.2.norm_final.bias:                                          256
* encoder.layers.3.self_attn.pos_bias_u:                                     256
* encoder.layers.3.self_attn.pos_bias_v:                                     256
* encoder.layers.3.self_attn.in_proj.weight:                              196608
* encoder.layers.3.self_attn.in_proj.bias:                                   768
* encoder.layers.3.self_attn.out_proj.weight:                              65536
* encoder.layers.3.self_attn.out_proj.bias:                                  256
* encoder.layers.3.self_attn.linear_pos.weight:                            65536
* encoder.layers.3.feed_forward.0.weight:                                 524288
* encoder.layers.3.feed_forward.0.bias:                                     2048
* encoder.layers.3.feed_forward.3.weight:                                 524288
* encoder.layers.3.feed_forward.3.bias:                                      256
* encoder.layers.3.feed_forward_macaron.0.weight:                         524288
* encoder.layers.3.feed_forward_macaron.0.bias:                             2048
* encoder.layers.3.feed_forward_macaron.3.weight:                         524288
* encoder.layers.3.feed_forward_macaron.3.bias:                              256
* encoder.layers.3.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.3.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.3.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.3.conv_module.depthwise_conv.bias:                          256
* encoder.layers.3.conv_module.norm.weight:                                  256
* encoder.layers.3.conv_module.norm.bias:                                    256
* encoder.layers.3.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.3.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.3.norm_ff_macaron.weight:                                   256
* encoder.layers.3.norm_ff_macaron.bias:                                     256
* encoder.layers.3.norm_ff.weight:                                           256
* encoder.layers.3.norm_ff.bias:                                             256
* encoder.layers.3.norm_mha.weight:                                          256
* encoder.layers.3.norm_mha.bias:                                            256
* encoder.layers.3.norm_conv.weight:                                         256
* encoder.layers.3.norm_conv.bias:                                           256
* encoder.layers.3.norm_final.weight:                                        256
* encoder.layers.3.norm_final.bias:                                          256
* encoder.layers.4.self_attn.pos_bias_u:                                     256
* encoder.layers.4.self_attn.pos_bias_v:                                     256
* encoder.layers.4.self_attn.in_proj.weight:                              196608
* encoder.layers.4.self_attn.in_proj.bias:                                   768
* encoder.layers.4.self_attn.out_proj.weight:                              65536
* encoder.layers.4.self_attn.out_proj.bias:                                  256
* encoder.layers.4.self_attn.linear_pos.weight:                            65536
* encoder.layers.4.feed_forward.0.weight:                                 524288
* encoder.layers.4.feed_forward.0.bias:                                     2048
* encoder.layers.4.feed_forward.3.weight:                                 524288
* encoder.layers.4.feed_forward.3.bias:                                      256
* encoder.layers.4.feed_forward_macaron.0.weight:                         524288
* encoder.layers.4.feed_forward_macaron.0.bias:                             2048
* encoder.layers.4.feed_forward_macaron.3.weight:                         524288
* encoder.layers.4.feed_forward_macaron.3.bias:                              256
* encoder.layers.4.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.4.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.4.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.4.conv_module.depthwise_conv.bias:                          256
* encoder.layers.4.conv_module.norm.weight:                                  256
* encoder.layers.4.conv_module.norm.bias:                                    256
* encoder.layers.4.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.4.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.4.norm_ff_macaron.weight:                                   256
* encoder.layers.4.norm_ff_macaron.bias:                                     256
* encoder.layers.4.norm_ff.weight:                                           256
* encoder.layers.4.norm_ff.bias:                                             256
* encoder.layers.4.norm_mha.weight:                                          256
* encoder.layers.4.norm_mha.bias:                                            256
* encoder.layers.4.norm_conv.weight:                                         256
* encoder.layers.4.norm_conv.bias:                                           256
* encoder.layers.4.norm_final.weight:                                        256
* encoder.layers.4.norm_final.bias:                                          256
* encoder.layers.5.self_attn.pos_bias_u:                                     256
* encoder.layers.5.self_attn.pos_bias_v:                                     256
* encoder.layers.5.self_attn.in_proj.weight:                              196608
* encoder.layers.5.self_attn.in_proj.bias:                                   768
* encoder.layers.5.self_attn.out_proj.weight:                              65536
* encoder.layers.5.self_attn.out_proj.bias:                                  256
* encoder.layers.5.self_attn.linear_pos.weight:                            65536
* encoder.layers.5.feed_forward.0.weight:                                 524288
* encoder.layers.5.feed_forward.0.bias:                                     2048
* encoder.layers.5.feed_forward.3.weight:                                 524288
* encoder.layers.5.feed_forward.3.bias:                                      256
* encoder.layers.5.feed_forward_macaron.0.weight:                         524288
* encoder.layers.5.feed_forward_macaron.0.bias:                             2048
* encoder.layers.5.feed_forward_macaron.3.weight:                         524288
* encoder.layers.5.feed_forward_macaron.3.bias:                              256
* encoder.layers.5.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.5.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.5.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.5.conv_module.depthwise_conv.bias:                          256
* encoder.layers.5.conv_module.norm.weight:                                  256
* encoder.layers.5.conv_module.norm.bias:                                    256
* encoder.layers.5.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.5.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.5.norm_ff_macaron.weight:                                   256
* encoder.layers.5.norm_ff_macaron.bias:                                     256
* encoder.layers.5.norm_ff.weight:                                           256
* encoder.layers.5.norm_ff.bias:                                             256
* encoder.layers.5.norm_mha.weight:                                          256
* encoder.layers.5.norm_mha.bias:                                            256
* encoder.layers.5.norm_conv.weight:                                         256
* encoder.layers.5.norm_conv.bias:                                           256
* encoder.layers.5.norm_final.weight:                                        256
* encoder.layers.5.norm_final.bias:                                          256
* encoder.layers.6.self_attn.pos_bias_u:                                     256
* encoder.layers.6.self_attn.pos_bias_v:                                     256
* encoder.layers.6.self_attn.in_proj.weight:                              196608
* encoder.layers.6.self_attn.in_proj.bias:                                   768
* encoder.layers.6.self_attn.out_proj.weight:                              65536
* encoder.layers.6.self_attn.out_proj.bias:                                  256
* encoder.layers.6.self_attn.linear_pos.weight:                            65536
* encoder.layers.6.feed_forward.0.weight:                                 524288
* encoder.layers.6.feed_forward.0.bias:                                     2048
* encoder.layers.6.feed_forward.3.weight:                                 524288
* encoder.layers.6.feed_forward.3.bias:                                      256
* encoder.layers.6.feed_forward_macaron.0.weight:                         524288
* encoder.layers.6.feed_forward_macaron.0.bias:                             2048
* encoder.layers.6.feed_forward_macaron.3.weight:                         524288
* encoder.layers.6.feed_forward_macaron.3.bias:                              256
* encoder.layers.6.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.6.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.6.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.6.conv_module.depthwise_conv.bias:                          256
* encoder.layers.6.conv_module.norm.weight:                                  256
* encoder.layers.6.conv_module.norm.bias:                                    256
* encoder.layers.6.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.6.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.6.norm_ff_macaron.weight:                                   256
* encoder.layers.6.norm_ff_macaron.bias:                                     256
* encoder.layers.6.norm_ff.weight:                                           256
* encoder.layers.6.norm_ff.bias:                                             256
* encoder.layers.6.norm_mha.weight:                                          256
* encoder.layers.6.norm_mha.bias:                                            256
* encoder.layers.6.norm_conv.weight:                                         256
* encoder.layers.6.norm_conv.bias:                                           256
* encoder.layers.6.norm_final.weight:                                        256
* encoder.layers.6.norm_final.bias:                                          256
* encoder.layers.7.self_attn.pos_bias_u:                                     256
* encoder.layers.7.self_attn.pos_bias_v:                                     256
* encoder.layers.7.self_attn.in_proj.weight:                              196608
* encoder.layers.7.self_attn.in_proj.bias:                                   768
* encoder.layers.7.self_attn.out_proj.weight:                              65536
* encoder.layers.7.self_attn.out_proj.bias:                                  256
* encoder.layers.7.self_attn.linear_pos.weight:                            65536
* encoder.layers.7.feed_forward.0.weight:                                 524288
* encoder.layers.7.feed_forward.0.bias:                                     2048
* encoder.layers.7.feed_forward.3.weight:                                 524288
* encoder.layers.7.feed_forward.3.bias:                                      256
* encoder.layers.7.feed_forward_macaron.0.weight:                         524288
* encoder.layers.7.feed_forward_macaron.0.bias:                             2048
* encoder.layers.7.feed_forward_macaron.3.weight:                         524288
* encoder.layers.7.feed_forward_macaron.3.bias:                              256
* encoder.layers.7.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.7.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.7.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.7.conv_module.depthwise_conv.bias:                          256
* encoder.layers.7.conv_module.norm.weight:                                  256
* encoder.layers.7.conv_module.norm.bias:                                    256
* encoder.layers.7.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.7.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.7.norm_ff_macaron.weight:                                   256
* encoder.layers.7.norm_ff_macaron.bias:                                     256
* encoder.layers.7.norm_ff.weight:                                           256
* encoder.layers.7.norm_ff.bias:                                             256
* encoder.layers.7.norm_mha.weight:                                          256
* encoder.layers.7.norm_mha.bias:                                            256
* encoder.layers.7.norm_conv.weight:                                         256
* encoder.layers.7.norm_conv.bias:                                           256
* encoder.layers.7.norm_final.weight:                                        256
* encoder.layers.7.norm_final.bias:                                          256
* encoder.layers.8.self_attn.pos_bias_u:                                     256
* encoder.layers.8.self_attn.pos_bias_v:                                     256
* encoder.layers.8.self_attn.in_proj.weight:                              196608
* encoder.layers.8.self_attn.in_proj.bias:                                   768
* encoder.layers.8.self_attn.out_proj.weight:                              65536
* encoder.layers.8.self_attn.out_proj.bias:                                  256
* encoder.layers.8.self_attn.linear_pos.weight:                            65536
* encoder.layers.8.feed_forward.0.weight:                                 524288
* encoder.layers.8.feed_forward.0.bias:                                     2048
* encoder.layers.8.feed_forward.3.weight:                                 524288
* encoder.layers.8.feed_forward.3.bias:                                      256
* encoder.layers.8.feed_forward_macaron.0.weight:                         524288
* encoder.layers.8.feed_forward_macaron.0.bias:                             2048
* encoder.layers.8.feed_forward_macaron.3.weight:                         524288
* encoder.layers.8.feed_forward_macaron.3.bias:                              256
* encoder.layers.8.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.8.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.8.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.8.conv_module.depthwise_conv.bias:                          256
* encoder.layers.8.conv_module.norm.weight:                                  256
* encoder.layers.8.conv_module.norm.bias:                                    256
* encoder.layers.8.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.8.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.8.norm_ff_macaron.weight:                                   256
* encoder.layers.8.norm_ff_macaron.bias:                                     256
* encoder.layers.8.norm_ff.weight:                                           256
* encoder.layers.8.norm_ff.bias:                                             256
* encoder.layers.8.norm_mha.weight:                                          256
* encoder.layers.8.norm_mha.bias:                                            256
* encoder.layers.8.norm_conv.weight:                                         256
* encoder.layers.8.norm_conv.bias:                                           256
* encoder.layers.8.norm_final.weight:                                        256
* encoder.layers.8.norm_final.bias:                                          256
* encoder.layers.9.self_attn.pos_bias_u:                                     256
* encoder.layers.9.self_attn.pos_bias_v:                                     256
* encoder.layers.9.self_attn.in_proj.weight:                              196608
* encoder.layers.9.self_attn.in_proj.bias:                                   768
* encoder.layers.9.self_attn.out_proj.weight:                              65536
* encoder.layers.9.self_attn.out_proj.bias:                                  256
* encoder.layers.9.self_attn.linear_pos.weight:                            65536
* encoder.layers.9.feed_forward.0.weight:                                 524288
* encoder.layers.9.feed_forward.0.bias:                                     2048
* encoder.layers.9.feed_forward.3.weight:                                 524288
* encoder.layers.9.feed_forward.3.bias:                                      256
* encoder.layers.9.feed_forward_macaron.0.weight:                         524288
* encoder.layers.9.feed_forward_macaron.0.bias:                             2048
* encoder.layers.9.feed_forward_macaron.3.weight:                         524288
* encoder.layers.9.feed_forward_macaron.3.bias:                              256
* encoder.layers.9.conv_module.pointwise_conv1.weight:                    131072
* encoder.layers.9.conv_module.pointwise_conv1.bias:                         512
* encoder.layers.9.conv_module.depthwise_conv.weight:                       7936
* encoder.layers.9.conv_module.depthwise_conv.bias:                          256
* encoder.layers.9.conv_module.norm.weight:                                  256
* encoder.layers.9.conv_module.norm.bias:                                    256
* encoder.layers.9.conv_module.pointwise_conv2.weight:                     65536
* encoder.layers.9.conv_module.pointwise_conv2.bias:                         256
* encoder.layers.9.norm_ff_macaron.weight:                                   256
* encoder.layers.9.norm_ff_macaron.bias:                                     256
* encoder.layers.9.norm_ff.weight:                                           256
* encoder.layers.9.norm_ff.bias:                                             256
* encoder.layers.9.norm_mha.weight:                                          256
* encoder.layers.9.norm_mha.bias:                                            256
* encoder.layers.9.norm_conv.weight:                                         256
* encoder.layers.9.norm_conv.bias:                                           256
* encoder.layers.9.norm_final.weight:                                        256
* encoder.layers.9.norm_final.bias:                                          256
* encoder.layers.10.self_attn.pos_bias_u:                                    256
* encoder.layers.10.self_attn.pos_bias_v:                                    256
* encoder.layers.10.self_attn.in_proj.weight:                             196608
* encoder.layers.10.self_attn.in_proj.bias:                                  768
* encoder.layers.10.self_attn.out_proj.weight:                             65536
* encoder.layers.10.self_attn.out_proj.bias:                                 256
* encoder.layers.10.self_attn.linear_pos.weight:                           65536
* encoder.layers.10.feed_forward.0.weight:                                524288
* encoder.layers.10.feed_forward.0.bias:                                    2048
* encoder.layers.10.feed_forward.3.weight:                                524288
* encoder.layers.10.feed_forward.3.bias:                                     256
* encoder.layers.10.feed_forward_macaron.0.weight:                        524288
* encoder.layers.10.feed_forward_macaron.0.bias:                            2048
* encoder.layers.10.feed_forward_macaron.3.weight:                        524288
* encoder.layers.10.feed_forward_macaron.3.bias:                             256
* encoder.layers.10.conv_module.pointwise_conv1.weight:                   131072
* encoder.layers.10.conv_module.pointwise_conv1.bias:                        512
* encoder.layers.10.conv_module.depthwise_conv.weight:                      7936
* encoder.layers.10.conv_module.depthwise_conv.bias:                         256
* encoder.layers.10.conv_module.norm.weight:                                 256
* encoder.layers.10.conv_module.norm.bias:                                   256
* encoder.layers.10.conv_module.pointwise_conv2.weight:                    65536
* encoder.layers.10.conv_module.pointwise_conv2.bias:                        256
* encoder.layers.10.norm_ff_macaron.weight:                                  256
* encoder.layers.10.norm_ff_macaron.bias:                                    256
* encoder.layers.10.norm_ff.weight:                                          256
* encoder.layers.10.norm_ff.bias:                                            256
* encoder.layers.10.norm_mha.weight:                                         256
* encoder.layers.10.norm_mha.bias:                                           256
* encoder.layers.10.norm_conv.weight:                                        256
* encoder.layers.10.norm_conv.bias:                                          256
* encoder.layers.10.norm_final.weight:                                       256
* encoder.layers.10.norm_final.bias:                                         256
* encoder.layers.11.self_attn.pos_bias_u:                                    256
* encoder.layers.11.self_attn.pos_bias_v:                                    256
* encoder.layers.11.self_attn.in_proj.weight:                             196608
* encoder.layers.11.self_attn.in_proj.bias:                                  768
* encoder.layers.11.self_attn.out_proj.weight:                             65536
* encoder.layers.11.self_attn.out_proj.bias:                                 256
* encoder.layers.11.self_attn.linear_pos.weight:                           65536
* encoder.layers.11.feed_forward.0.weight:                                524288
* encoder.layers.11.feed_forward.0.bias:                                    2048
* encoder.layers.11.feed_forward.3.weight:                                524288
* encoder.layers.11.feed_forward.3.bias:                                     256
* encoder.layers.11.feed_forward_macaron.0.weight:                        524288
* encoder.layers.11.feed_forward_macaron.0.bias:                            2048
* encoder.layers.11.feed_forward_macaron.3.weight:                        524288
* encoder.layers.11.feed_forward_macaron.3.bias:                             256
* encoder.layers.11.conv_module.pointwise_conv1.weight:                   131072
* encoder.layers.11.conv_module.pointwise_conv1.bias:                        512
* encoder.layers.11.conv_module.depthwise_conv.weight:                      7936
* encoder.layers.11.conv_module.depthwise_conv.bias:                         256
* encoder.layers.11.conv_module.norm.weight:                                 256
* encoder.layers.11.conv_module.norm.bias:                                   256
* encoder.layers.11.conv_module.pointwise_conv2.weight:                    65536
* encoder.layers.11.conv_module.pointwise_conv2.bias:                        256
* encoder.layers.11.norm_ff_macaron.weight:                                  256
* encoder.layers.11.norm_ff_macaron.bias:                                    256
* encoder.layers.11.norm_ff.weight:                                          256
* encoder.layers.11.norm_ff.bias:                                            256
* encoder.layers.11.norm_mha.weight:                                         256
* encoder.layers.11.norm_mha.bias:                                           256
* encoder.layers.11.norm_conv.weight:                                        256
* encoder.layers.11.norm_conv.bias:                                          256
* encoder.layers.11.norm_final.weight:                                       256
* encoder.layers.11.norm_final.bias:                                         256
* encoder_output_layer.1.weight:                                           22272
* encoder_output_layer.1.bias:                                                87
================================================================================
Total: 32081863
================================================================================
No ali_model
epoch 0, learning rate 0
batch 0, epoch 0/10 global average objf: 1.619938 over 2447.0 frames (100.0% kept), current batch average objf: 1.619937 over 2447 frames (100.0% kept) avg time waiting for batch 35.985s
Reducer buckets have been rebuilt in this iteration.
batch 10, epoch 0/10 global average objf: 1.642242 over 26508.0 frames (100.0% kept), current batch average objf: 1.791056 over 2442 frames (100.0% kept) avg time waiting for batch 3.604s
batch 20, epoch 0/10 global average objf: 1.640815 over 50314.0 frames (100.0% kept), current batch average objf: 1.537894 over 2462 frames (100.0% kept) avg time waiting for batch 1.810s
batch 30, epoch 0/10 global average objf: 1.614365 over 74477.0 frames (100.0% kept), current batch average objf: 1.575541 over 2453 frames (100.0% kept) avg time waiting for batch 1.209s
batch 40, epoch 0/10 global average objf: 1.605720 over 98420.0 frames (100.0% kept), current batch average objf: 1.542489 over 2446 frames (100.0% kept) avg time waiting for batch 0.909s
batch 50, epoch 0/10 global average objf: 1.578103 over 122472.0 frames (100.0% kept), current batch average objf: 1.398966 over 2402 frames (100.0% kept) avg time waiting for batch 0.732s
batch 60, epoch 0/10 global average objf: 1.556495 over 146613.0 frames (100.0% kept), current batch average objf: 1.503467 over 2398 frames (100.0% kept) avg time waiting for batch 0.611s
batch 70, epoch 0/10 global average objf: 1.531411 over 170627.0 frames (100.0% kept), current batch average objf: 1.130030 over 2441 frames (100.0% kept) avg time waiting for batch 0.524s
batch 80, epoch 0/10 global average objf: 1.516437 over 194708.0 frames (100.0% kept), current batch average objf: 1.392735 over 2398 frames (100.0% kept) avg time waiting for batch 0.462s
batch 90, epoch 0/10 global average objf: 1.502252 over 218580.0 frames (100.0% kept), current batch average objf: 1.275368 over 2447 frames (100.0% kept) avg time waiting for batch 0.411s
batch 100, epoch 0/10 global average objf: 1.487170 over 242585.0 frames (100.0% kept), current batch average objf: 1.220927 over 2385 frames (100.0% kept) avg time waiting for batch 0.372s
batch 110, epoch 0/10 global average objf: 1.477120 over 266534.0 frames (100.0% kept), current batch average objf: 1.491579 over 2346 frames (100.0% kept) avg time waiting for batch 0.345s
batch 120, epoch 0/10 global average objf: 1.463303 over 290466.0 frames (100.0% kept), current batch average objf: 1.416170 over 2393 frames (100.0% kept) avg time waiting for batch 0.320s
batch 130, epoch 0/10 global average objf: 1.455006 over 314427.0 frames (100.0% kept), current batch average objf: 1.571258 over 2432 frames (100.0% kept) avg time waiting for batch 0.295s
batch 140, epoch 0/10 global average objf: 1.444346 over 338586.0 frames (100.0% kept), current batch average objf: 1.381285 over 2352 frames (100.0% kept) avg time waiting for batch 0.275s
batch 150, epoch 0/10 global average objf: 1.421460 over 366533.0 frames (100.0% kept), current batch average objf: 0.327271 over 6510 frames (100.0% kept) avg time waiting for batch 0.257s
batch 160, epoch 0/10 global average objf: 1.410988 over 390632.0 frames (100.0% kept), current batch average objf: 1.293839 over 2458 frames (100.0% kept) avg time waiting for batch 0.241s
batch 170, epoch 0/10 global average objf: 1.400501 over 414438.0 frames (100.0% kept), current batch average objf: 1.355720 over 2462 frames (100.0% kept) avg time waiting for batch 0.227s
batch 180, epoch 0/10 global average objf: 1.391359 over 438097.0 frames (100.0% kept), current batch average objf: 1.189415 over 2277 frames (100.0% kept) avg time waiting for batch 0.215s
batch 190, epoch 0/10 global average objf: 1.386311 over 462019.0 frames (100.0% kept), current batch average objf: 1.143106 over 2384 frames (99.9% kept) avg time waiting for batch 0.203s
batch 200, epoch 0/10 global average objf: 1.381136 over 485989.0 frames (100.0% kept), current batch average objf: 1.216378 over 2442 frames (100.0% kept) avg time waiting for batch 0.193s
/home/hltcoe/aarora/lhotse/lhotse/dataset/sampling.py:316: UserWarning: The first cut drawn in batch collection violates the max_frames or max_cuts constraints - we'll return it anyway. Consider increasing max_frames/max_cuts.
  warnings.warn("The first cut drawn in batch collection violates the max_frames or max_cuts "
Traceback (most recent call last):
  File "mmi_att_transformer_train.py", line 664, in <module>
    main()
  File "mmi_att_transformer_train.py", line 657, in main
    mp.spawn(run, args=(world_size, args), nprocs=world_size, join=True)
  File "/home/hltcoe/aarora/miniconda3/envs/k2/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/hltcoe/aarora/miniconda3/envs/k2/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/home/hltcoe/aarora/miniconda3/envs/k2/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 150, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/hltcoe/aarora/miniconda3/envs/k2/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/home/hltcoe/aarora/snowfall/egs/safet/asr/simple_v1/mmi_att_transformer_train.py", line 575, in run
    objf, valid_objf, global_batch_idx_train = train_one_epoch(
  File "/home/hltcoe/aarora/snowfall/egs/safet/asr/simple_v1/mmi_att_transformer_train.py", line 293, in train_one_epoch
    total_valid_objf, total_valid_frames, total_valid_all_frames = get_validation_objf(
  File "/home/hltcoe/aarora/snowfall/egs/safet/asr/simple_v1/mmi_att_transformer_train.py", line 161, in get_validation_objf
    objf, frames, all_frames = get_objf(
  File "/home/hltcoe/aarora/snowfall/egs/safet/asr/simple_v1/mmi_att_transformer_train.py", line 104, in get_objf
    mmi_loss, tot_frames, all_frames = loss_fn(nnet_output, texts, supervision_segments)
  File "/home/hltcoe/aarora/miniconda3/envs/k2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/hltcoe/aarora/snowfall/snowfall/objectives/mmi.py", line 38, in forward
    dense_fsa_vec = k2.DenseFsaVec(nnet_output, supervision_segments)
  File "/home/hltcoe/aarora/miniconda3/envs/k2/lib/python3.8/site-packages/k2/dense_fsa_vec.py", line 85, in __init__
    assert duration > 0
AssertionError

/cm/local/apps/uge/var/spool/r8n04/job_scripts/9177643: ended at Wed May 19 20:13:25 EDT 2021
